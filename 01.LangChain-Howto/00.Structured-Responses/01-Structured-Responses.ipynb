{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain — Structured Outputs with Ollama (DeepSeek R1 8B)\n",
        "\n",
        "This notebook demonstrates how to return structured data from a model using LangChain's `with_structured_output` with `ChatOllama` and the `deepseek-r1:8b` model running locally via Ollama.\n",
        "\n",
        "Reference: [LangChain — Structured Output](https://python.langchain.com/docs/how_to/structured_outputs/)\n",
        "\n",
        "Assumptions:\n",
        "- Ollama is running locally at `http://localhost:11434`\n",
        "- The model `deepseek-r1:8b` is installed (run `ollama pull deepseek-r1:8b` if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Optional: environment setup\n",
        "python -V\n",
        "pip install -qU langchain langchain-core langchain-ollama pydantic rich\n",
        "# If you haven't yet: `ollama pull deepseek-r1:8b`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Literal, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Basic connectivity check to Ollama\n",
        "llm = ChatOllama(model=\"deepseek-r1:8b\", temperature=0)\n",
        "resp = llm.invoke(\"Return the word 'ready'.\")\n",
        "print(resp.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic schema: Example 1 — Simple entity\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline of the joke\")\n",
        "\n",
        "# Use JSON output mode to make the model emit pure JSON\n",
        "json_llm = ChatOllama(model=\"deepseek-r1:8b\", temperature=0, format=\"json\")\n",
        "\n",
        "# Bind structured output to the model\n",
        "structured_llm = json_llm.with_structured_output(Joke)\n",
        "\n",
        "joke: Joke = structured_llm.invoke(\n",
        "    \"Tell me a short, clean programming joke.\"\n",
        ")\n",
        "\n",
        "print(joke)\n",
        "print(\"\\nAs Python dict:\", joke.model_dump())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic schema: Example 2 — Nested structure and enums\n",
        "class Task(BaseModel):\n",
        "    title: str\n",
        "    priority: Literal[\"low\", \"medium\", \"high\"]\n",
        "    due_days: int = Field(ge=0, description=\"Days from today until due\")\n",
        "\n",
        "class TodoList(BaseModel):\n",
        "    owner: str\n",
        "    tasks: List[Task]\n",
        "    notes: Optional[str] = None\n",
        "\n",
        "structured_todos = json_llm.with_structured_output(TodoList)\n",
        "\n",
        "todos: TodoList = structured_todos.invoke(\n",
        "    \"Create a to-do list for a developer preparing a LangChain demo. Return 3 tasks with varying priorities and realistic due days.\"\n",
        ")\n",
        "\n",
        "print(todos)\n",
        "print(\"\\nJSON:\")\n",
        "print(todos.model_dump_json(indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3 — JSON Schema (dict) without Pydantic\n",
        "# You can pass a raw JSON Schema and get back a Python dict.\n",
        "event_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\", \"description\": \"Event name\"},\n",
        "        \"date\": {\"type\": \"string\", \"description\": \"ISO date\"},\n",
        "        \"location\": {\"type\": \"string\"},\n",
        "        \"attendees\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\"type\": \"string\"},\n",
        "            \"description\": \"List of attendee names\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"name\", \"date\", \"location\", \"attendees\"],\n",
        "    \"additionalProperties\": False\n",
        "}\n",
        "\n",
        "structured_event = json_llm.with_structured_output(event_schema)\n",
        "\n",
        "event = structured_event.invoke(\n",
        "    \"Create a small event for a local Python meetup with 3 attendees.\"\n",
        ")\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(event)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- If `deepseek-r1:8b` returns reasoning text, using `format=\"json\"` helps ensure pure JSON for structured output.\n",
        "- For Pydantic models, the result is an instance with `.model_dump()` / `.model_dump_json()`.\n",
        "- For raw JSON Schema, the result is a plain Python `dict`.\n",
        "- Ensure Ollama is running: `ollama serve` and model pulled: `ollama pull deepseek-r1:8b`.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
