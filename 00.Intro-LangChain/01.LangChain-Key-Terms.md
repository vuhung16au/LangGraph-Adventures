## Large Language Model (LLM)

Definition

An AI model that generates text given text input. In LangChain, models are invoked like functions and can be used standalone or as part of a pipeline.

Example

Generate a summary from a paragraph of text.

## Chat Models

Definition

LLMs that take a sequence of messages (with roles like user/assistant/system) and return a message. Optimized for multi-turn dialogue.

Example

Answer a user question while considering the prior conversation.

## Messages

Definition

The building blocks passed to chat models (e.g., system, user, AI messages). They carry both content and role.

Example

Send a system instruction, then a user question; receive an AI reply.

## Chat History

Definition

The ordered list of past messages in a conversation. Used to give models context across turns.

Example

Keep the last 10 messages so the assistant remembers earlier preferences.

## Memory

Definition

Mechanisms for persisting information across runs or sessions (e.g., conversation buffers, summaries, entity memories).

Example

Store that the user’s name is "Alex" and reuse it in future replies.

## Checkpoint (Checkpointer)

Definition

A mechanism for saving and restoring the state of a LangChain application at specific points in time. Checkpoints capture the complete state including memory, conversation history, tool states, and execution context, allowing applications to resume from where they left off.

Thread

A persistent conversation session that maintains state across multiple interactions. Threads work with checkpoints to enable long-running conversations and workflows that can be paused, saved, and resumed. They store the complete conversation history, tool execution states, and any custom data, making them essential for building conversational AI applications that remember context over time.

Example

Save a checkpoint after processing 100 documents, then restore it later to continue processing from document 101 without losing previous work. Create a thread for a customer support conversation that persists across multiple sessions, allowing the AI to remember previous interactions and maintain context.

## Graph (LangGraph)

Definition

The control-flow of an application expressed as nodes and edges operating over a shared state. Input state enters the graph, flows through nodes (which read/update state), and exits with an updated state.

Example

Route a user request through nodes for classification → retrieval → answer generation using a single evolving state object.

## Sub-graph (LangGraph)

Definition

A reusable graph nested inside a larger graph. Sub-graphs encapsulate a portion of control flow and state updates so they can be composed, tested, and reused like functions.

Example

Create a sub-graph for “retrieve-and-rerank” and call it from multiple agents or routes within the main graph.

## Super-steps (LangGraph)

Definition

Execution boundaries within a graph. Each sequential node advances to a new super-step, while nodes run in parallel share the same super-step.

Example

Fan out to multiple retrievers in parallel within one super-step, then consolidate results in the next super-step.

## StateSnapshot (LangGraph)

Definition

The packaged state plus metadata captured at each super-step. This is the type stored in checkpoints and used to resume or inspect execution.

Example

Inspect a `StateSnapshot` to see the current conversation history, selected tools, and the next node to execute.

## Graph.get_state() (LangGraph)

Definition

Returns the most recent `StateSnapshot` (current checkpoint) for a given thread or run.

Example

Fetch the latest checkpoint to display the current state in a UI or to resume processing.

## Graph.get_state_history() (LangGraph)

Definition

Returns the list of all `StateSnapshot` objects for a thread, representing the full sequence of super-steps taken.

Example

Render an execution timeline by iterating over the state history to visualize how state evolved across nodes.

## Tools

Definition

External functions the model can call (with a name, description, and input schema) to act in the world or fetch data.

Example

A "Weather" tool that returns current conditions for a city.

## Tool Calling

Definition

Model outputs that include structured requests to invoke tools. The model decides when and how to call a tool.

Example

The model calls a calculator tool to compute 23 × 47 before responding.

## Structured Output

Definition

Techniques to make models return data in a strict schema (e.g., JSON conforming to a Pydantic model) for reliable downstream use.

Example

Return {"title": str, "tags": list[str]} for a blog post request.

## Prompt Templates

Definition

Reusable prompts that separate static instructions from variable inputs. Helpful for versioning and consistency.

Example

"Write a {tone} summary of: {text}" where tone/text are variables.

## Chains

Definition

Compositions of multiple steps (prompts, models, tools, transforms) into a single callable pipeline.

Example

Extract keywords → retrieve docs → answer with citations.

## Long-term Memory

Definition

Persistent storage mechanisms that maintain information across extended periods, sessions, or application restarts. Unlike short-term memory (conversation buffers), long-term memory can store user preferences, learned facts, historical interactions, and other data that should persist beyond the current conversation context.

Example

Store a user's dietary preferences, favorite topics, and communication style to personalize future interactions even after weeks or months of inactivity.

## Map-Reduce (LangChain)

Definition

A scalable processing pattern where a task is split into independent "map" operations over chunks (e.g., documents), then combined by a "reduce" step. Often used for summarization, scoring, or extraction across large corpora.

Example

Summarize thousands of documents by first generating per-chunk summaries (map), then merging them into a single coherent summary (reduce), optionally with hierarchical or iterative reductions.

## LangChain Expression Language (LCEL)

Definition

A declarative way to compose components (prompts, models, retrievers, parsers) with built-in streaming, batching, and observability.

Example

Prompt → LLM → JSON parser composed into one runnable.

## Runnables

Definition

The core interface for executable components in LangChain. Anything that takes input and produces output (LLMs, prompts, retrievers) implements it. This standardized interface makes components easily interchangeable and composable for building complex applications.

Key Methods

- invoke: Call the runnable with a single input and return a single output.
- batch: Call the runnable with multiple inputs at once and return outputs in order.
- stream: Stream tokens or incremental results as they are generated for real-time experiences.

Example

Treat a retriever or LLM as a function you can invoke, stream, or batch.

## Streaming

Definition

Emit partial tokens or intermediate results as they are generated to improve responsiveness and UX.

Example

Display an answer word-by-word while the model generates it.

## Document Loaders

Definition

Utilities to load raw sources (files, web pages, databases) into standardized Document objects with content and metadata.

Example

Load all PDFs from a folder into documents for later indexing.

## Text Splitters

Definition

Split long documents into manageable chunks optimized for retrieval and model context windows.

Example

Chunk a 50-page PDF into ~500-token overlapping segments.

## Embeddings

Definition

Numeric vector representations of text or images used for similarity search and clustering.

Example

Embed product descriptions to find similar items.

## Vector Stores

Definition

Databases specialized for storing embeddings and performing fast similarity search with metadata filtering.

Example

Upsert embeddings into a vector DB and query top-k similar chunks.

## Retrievers

Definition

Interfaces that, given a query, return relevant documents. Often backed by vector stores, BM25, or hybrid search.

Example

Return the 5 most similar chunks for a user question.

## Retrieval Augmented Generation (RAG)

Definition

Patterns that retrieve external context and feed it to the model to ground answers and reduce hallucinations.

Example

Retrieve policy docs and answer a compliance question with citations.

## Agents

Definition

Systems where the model decides which actions (tools) to take in what order based on intermediate results.

Example

Research agent that searches the web, reads pages, then summarizes findings.

## Output Parsers

Definition

Components that convert model text into structured Python objects (JSON, enums, numbers). Less needed when using tool calling/structured outputs, but still useful.

Example

Parse an LLM’s table into a list of rows with typed fields.

## Example Selectors (Few-Shot)

Definition

Choose the most relevant examples to include in a prompt for better guidance and accuracy.

Example

Pick the top 3 similar Q&A pairs to condition the model.

## Callbacks & Tracing

Definition

Hooks and observability to log, stream, and trace each step of a chain/agent for debugging and monitoring.

Example

Record latency and inputs/outputs for every component call.

## Evaluation

Definition

Methods to assess quality (unit tests, golden sets, rubric-based LLM judges, metrics) across tasks and datasets.

Example

Score answers for helpfulness and citation accuracy before deployment.

## Multimodality

Definition

Support for inputs/outputs beyond text (images, audio, video). Enables vision-QA, captioning, and mixed-media agents.

Example

Ask: "Describe this image" and supply a photo along with text.
